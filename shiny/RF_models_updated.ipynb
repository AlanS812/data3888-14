{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c345b3-e438-4fb1-abd6-365365bfd958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autofocus - big problem read into it:  \n",
    "# look at how the different cell types change with the blurring \n",
    "# the distribution of confidence, but same results? plot the probabilities \n",
    "# try on the different cell groupings \n",
    "# non blur training, blur testing \n",
    "\n",
    "# My stuff \n",
    "# torch.save - to save the outputs of the model \n",
    "# maybe neaten code and turn into functions\n",
    "\n",
    "# - metadata useful?\n",
    "# - HOG: look at parameters \n",
    "# - RF: labels? encoding? what is my model doing?\n",
    "\n",
    "# PCA\n",
    "# reduces dimensionality by projecting to directions of max variance\n",
    "# good at compact global representation\n",
    "# but doesn't capture spatial strucure (e.g. edges, orientation)\n",
    "# good if images are uniform size, when you want to reduce feature count drastically \n",
    "# e.g. many raw pixels to components\n",
    "# good for random forest or SVM that don't like ultra high-dimension\n",
    "# PCA is a statistical abstraction of the whole image, HOG is hand-crafted structure\n",
    "\n",
    "# HOG\n",
    "# captured edge direction and local shape info\n",
    "# good at preserving spatial patterns (boundaries, contours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c4313-89ae-498a-b829-8ad29c17cc95",
   "metadata": {},
   "source": [
    "## RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2063ed4-166c-4293-8692-b2f33f7edb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Immune', 0), ('Other', 1), ('Stromal', 2), ('Tumour', 3)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import shiny_data\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageEnhance\n",
    "from skimage.feature import hog\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "# Suppress specific warning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# 0 Immune, 1 Other, 2 Stromal, 3 Tumour\n",
    "Xmat_train, Xmat_val, Xmat_tests_0, Xmat_tests_1, Xmat_tests_2, y_train_enc, y_val_enc, y_tests_enc_0, y_tests_enc_1, y_tests_enc_2 = shiny_data.load_split_images()\n",
    "\n",
    "# reassigning to just use first one for shiny\n",
    "Xmat_test = Xmat_tests_0\n",
    "y_test_enc = y_tests_enc_0\n",
    "\n",
    "# HAVE TO REDO THESE FUNCTIONS IN THE SHINY FILE\n",
    "# 50\n",
    "# Xmat_train, Xmat_val, Xmat_test, y_train_enc, y_val_enc, y_test_enc = shiny_data.load_split_images_50()\n",
    "# 100\n",
    "# Xmat_train, Xmat_val, Xmat_test, y_train_enc, y_val_enc, y_test_enc = shiny_data.load_split_images_100()\n",
    "\n",
    "def store_results(results, y_true, y_pred, probs=None, blur=0, noise=0, hog=False, pca=False, **extra_metrics):        \n",
    "    entry = {\n",
    "        \"blur_size\": blur,\n",
    "        \"noise_level\": noise,\n",
    "        #\"HOG\": hog,\n",
    "        #\"PCA\": pca, # only doing the PCA one for shiny\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1\": f1_score(y_true, y_pred, average='macro'),\n",
    "        \"precision\": precision_score(y_true, y_pred, average='macro'),\n",
    "        \"recall\": recall_score(y_true, y_pred, average='macro'),\n",
    "    }\n",
    "\n",
    "    f1_classes = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    precision_classes = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    recall_classes = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "\n",
    "    label_map = {\n",
    "        0: \"immune\",\n",
    "        1: \"other\",\n",
    "        2: \"stromal\",\n",
    "        3: \"tumour\"\n",
    "    }\n",
    "    \n",
    "    for i in range(4):\n",
    "        label = label_map[i]\n",
    "        entry[f\"f1_{label}\"] = f1_classes[i]\n",
    "        entry[f\"precision_{label}\"] = precision_classes[i]\n",
    "        entry[f\"recall_{label}\"] = recall_classes[i]\n",
    "    \n",
    "    if probs is not None:\n",
    "        confidences = np.max(probs, axis=1)\n",
    "        entry[\"confidence_overall\"] = np.mean(confidences)\n",
    "        for cls in range(4):\n",
    "            label = label_map[cls]\n",
    "            cls_conf = confidences[y_pred == cls]\n",
    "            entry[f\"confidence_{label}_avg\"] = np.mean(cls_conf) if len(cls_conf) > 0 else np.nan\n",
    "            entry[f\"confidence_{label}_std\"] = np.std(cls_conf) if len(cls_conf) > 0 else np.nan\n",
    "    \n",
    "    total_preds = len(y_pred)\n",
    "    for cls in range(4):\n",
    "        label = label_map[cls]\n",
    "        entry[f\"count_pred_{label}\"] = np.sum(y_pred == cls)\n",
    "\n",
    "    # Store actual and predicted labels and the confusion matrix as strings\n",
    "    entry[\"confusion_matrix\"] = str(confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3]).tolist())\n",
    "\n",
    "    entry.update(extra_metrics)\n",
    "    results.append(entry)\n",
    "\n",
    "def extract_hog_features(images):\n",
    "    hog_features = []\n",
    "    for img in images:\n",
    "        if img.shape[-1] == 3:\n",
    "            img = Image.fromarray((img * 255).astype(np.uint8)).convert(\"L\")\n",
    "            img = np.array(img)\n",
    "        features = hog(img, pixels_per_cell=(8, 8), cells_per_block=(2, 2), feature_vector=True)\n",
    "        hog_features.append(features)\n",
    "    return np.array(hog_features)\n",
    "\n",
    "# use shiny version (the same but cleaner), and same for hog thing if possible?\n",
    "def apply_noise(images, mean=0, std=10, seed=3888):\n",
    "    if std > 0:\n",
    "        np.random.seed(seed)  # Consistent noise for all images in this batch\n",
    "        noise = np.random.normal(mean, std, images.shape)\n",
    "        noisy_images = images + noise\n",
    "        noisy_images = np.clip(noisy_images, 0, 255).astype(np.uint8)\n",
    "        return noisy_images\n",
    "    else:\n",
    "        return images\n",
    "\n",
    "X_train_flat = Xmat_train.reshape(Xmat_train.shape[0], -1)\n",
    "X_val_flat   = Xmat_val.reshape(Xmat_val.shape[0], -1)\n",
    "X_test_flat  = Xmat_test.reshape(Xmat_test.shape[0], -1)\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "X_train_pca = pca.fit_transform(X_train_flat)\n",
    "\n",
    "# HOG not used for shiny\n",
    "# X_train_hog = extract_hog_features(Xmat_train) \n",
    "\n",
    "# results = []\n",
    "# df = pd.DataFrame(results)  # no explicit column list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421dca65-206b-4ba2-8948-a5f6ec23c887",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c49c7b52-0d43-4447-80e1-fa1190ceb972",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pca = RandomForestClassifier(n_estimators=100, random_state=3888)\n",
    "model_pca.fit(X_train_pca, y_train_enc)\n",
    "\n",
    "# baseline\n",
    "X_test_pca = pca.transform(X_test_flat)\n",
    "y_pred = model_pca.predict(X_test_pca)\n",
    "y_probs = model_pca.predict_proba(X_test_pca)\n",
    "store_results(results, y_test_enc, y_pred, probs=y_probs)\n",
    "\n",
    "for radius in [1, 3, 5, 7, 9, 19]: # blur\n",
    "    X_blur = shiny_data.apply_blur(Xmat_test, radius)\n",
    "    X_test_flat = X_blur.reshape(X_blur.shape[0], -1)\n",
    "    X_test_pca = pca.transform(X_test_flat)\n",
    "    y_pred = model_pca.predict(X_test_pca)\n",
    "    y_probs = model_pca.predict_proba(X_test_pca)\n",
    "    store_results(results, y_test_enc, y_pred, blur=radius, probs=y_probs)\n",
    "\n",
    "for noise_std in [1, 3, 5, 10, 20, 30]:\n",
    "    seed = 3888 + noise_std  # same seed logic as baseline\n",
    "    X_noisy = apply_noise(Xmat_test, std=noise_std, seed=seed)\n",
    "    X_test_flat = X_noisy.reshape(X_noisy.shape[0], -1)\n",
    "    X_test_pca = pca.transform(X_test_flat)\n",
    "    y_pred = model_pca.predict(X_test_pca)\n",
    "    y_probs = model_pca.predict_proba(X_test_pca)\n",
    "    store_results(results, y_test_enc, y_pred, noise=noise_std, probs=y_probs)\n",
    "\n",
    "for radius in [1, 3, 5, 7, 9, 19]: # blur\n",
    "    for noise_std in [1, 3, 5, 10, 20, 30]:\n",
    "        combo_seed = 10000 + (radius * 100) + noise_std\n",
    "        X_blur = shiny_data.apply_blur(Xmat_test, radius)\n",
    "        X_combo = apply_noise(X_blur, std=noise_std, seed=combo_seed)\n",
    "        X_test_flat = X_combo.reshape(X_combo.shape[0], -1)\n",
    "        X_test_pca = pca.transform(X_test_flat)\n",
    "        y_pred = model_pca.predict(X_test_pca)\n",
    "        y_probs = model_pca.predict_proba(X_test_pca)\n",
    "        store_results(results, y_test_enc, y_pred, blur=radius, noise=noise_std, probs=y_probs)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"rf_augmented_metrics.csv\", index=False) # update to 100 here and above as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43a519f9-b77e-4e57-be85-74ba50623171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_pca_model.joblib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "model_pca = RandomForestClassifier(n_estimators=100, random_state=3888)\n",
    "model_pca.fit(X_train_pca, y_train_enc)\n",
    "joblib.dump(pca, \"Base_pca.joblib\")\n",
    "joblib.dump(model_pca, \"rf_pca_model.joblib\")  # âœ… save the RF model too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7934a66-bd16-47ac-b6e3-24cd7c740c26",
   "metadata": {},
   "source": [
    "## RF no HOG or PCA - NOT RUN/UPDATED FOR SHINY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10876172-15fe-440d-865c-8aeb3012de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=3888)\n",
    "model.fit(X_train_flat, y_train_enc)\n",
    "y_pred = model.predict(X_test_flat)\n",
    "y_probs = model.predict_proba(X_test_flat)\n",
    "store_results(results, y_test_enc, y_pred, blur=0, noise=0, probs=y_probs)\n",
    "\n",
    "# BLUR TESTS\n",
    "for radius in [1, 3, 5, 7, 9, 19]:\n",
    "    X_blur_test = shiny_data.apply_blur(Xmat_test, radius)\n",
    "    X_test_flat = X_blur_test.reshape(X_blur_test.shape[0], -1)\n",
    "    y_pred = model.predict(X_test_flat)\n",
    "    y_probs = model.predict_proba(X_test_flat)\n",
    "    store_results(results, y_test_enc, y_pred, blur=radius, noise=0, probs=y_probs)\n",
    "\n",
    "# NOISE TESTS\n",
    "for noise_std in [1, 3, 5, 10, 20, 30]:\n",
    "    seed = 3888 + noise_std  # Ensure different but reproducible seed per level\n",
    "    X_noisy_test = apply_noise(Xmat_test, std=noise_std, seed=seed)\n",
    "    X_test_flat = X_noisy_test.reshape(X_noisy_test.shape[0], -1)\n",
    "    y_pred = model.predict(X_test_flat)\n",
    "    y_probs = model.predict_proba(X_test_flat)\n",
    "    store_results(results, y_test_enc, y_pred, blur=0, noise=noise_std, probs=y_probs)\n",
    "\n",
    "# BLUR + NOISE TESTS\n",
    "for radius in [1, 3, 5, 7, 9, 19]:\n",
    "    for noise_std in [1, 3, 5, 10, 20, 30]:\n",
    "        X_blur = shiny_data.apply_blur(Xmat_test, radius)\n",
    "        combo_seed = 10000 + (radius * 100) + noise_std  # Unique seed per combo\n",
    "        X_combo = apply_noise(X_blur, std=noise_std, seed=combo_seed)\n",
    "        X_test_flat = X_combo.reshape(X_combo.shape[0], -1)\n",
    "        y_pred = model.predict(X_test_flat)\n",
    "        y_probs = model.predict_proba(X_test_flat)\n",
    "        store_results(results, y_test_enc, y_pred, blur=radius, noise=noise_std, probs=y_probs)\n",
    "\n",
    "# Show progress\n",
    "#df = pd.DataFrame(results)\n",
    "#print(\"\\n=== After Blur and Noise Augmentations ===\")\n",
    "#cols = [\n",
    "#    \"HOG\", \"PCA\", \"Blur\", \"Noise\",\n",
    "#    \"accuracy\", \"f1\", \"precision\", \"recall\",\n",
    "#    \"f1_class_0\", \"f1_class_1\", \"f1_class_2\", \"f1_class_3\",\n",
    "#    \"precision_class_0\", \"precision_class_1\", \"precision_class_2\", \"precision_class_3\",\n",
    "#    \"recall_class_0\", \"recall_class_1\", \"recall_class_2\", \"recall_class_3\",\n",
    "#    \"avg_conf\", \"avg_conf_class_0\", \"avg_conf_class_1\", \"avg_conf_class_2\", \"avg_conf_class_3\",\n",
    "#    \"count_pred_class_0\", \"count_pred_class_1\", \"count_pred_class_2\", \"count_pred_class_3\"\n",
    "#]\n",
    "\n",
    "#display(df[cols].sort_values(by=[\"PCA\", \"Blur\", \"Noise\"]).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1266d4d2-1727-4ac8-bda8-3564f017da99",
   "metadata": {},
   "source": [
    "## HOG - NOT RUN/UPDATED FOR SHINY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d956f9-0c01-4c3e-b805-55d861c86531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HOG MODEL ===\n",
    "model_hog = RandomForestClassifier(n_estimators=100, random_state=3888)\n",
    "model_hog.fit(X_train_hog, y_train_enc)\n",
    "\n",
    "# BASELINE HOG TEST\n",
    "X_test_hog = extract_hog_features(Xmat_test)\n",
    "y_pred = model_hog.predict(X_test_hog)\n",
    "probs = model_hog.predict_proba(X_test_hog)\n",
    "store_results(results, y_test_enc, y_pred, probs=probs, hog=True)\n",
    "\n",
    "# BLUR + HOG\n",
    "for radius in [1, 3, 5, 7, 9, 19]:\n",
    "    X_blur = shiny_data.apply_blur(Xmat_test, radius)\n",
    "    X_test_hog = extract_hog_features(X_blur)\n",
    "    y_pred = model_hog.predict(X_test_hog)\n",
    "    probs = model_hog.predict_proba(X_test_hog)\n",
    "    store_results(results, y_test_enc, y_pred, probs=probs, blur=radius, hog=True)\n",
    "\n",
    "# NOISE + HOG\n",
    "for noise_std in [1, 3, 5, 10, 20, 30]:\n",
    "    seed = 3888 + noise_std  # ensures consistent noise for this std level\n",
    "    X_noisy = apply_noise(Xmat_test, std=noise_std, seed=seed)\n",
    "    X_test_hog = extract_hog_features(X_noisy)\n",
    "    y_pred = model_hog.predict(X_test_hog)\n",
    "    probs = model_hog.predict_proba(X_test_hog)\n",
    "    store_results(results, y_test_enc, y_pred, probs=probs, noise=noise_std, hog=True)\n",
    "\n",
    "# BLUR + NOISE + HOG\n",
    "for radius in [1, 3, 5, 7, 9, 19]:\n",
    "    for noise_std in [1, 3, 5, 10, 20, 30]:\n",
    "        combo_seed = 10000 + (radius * 100) + noise_std\n",
    "        X_blur = shiny_data.apply_blur(Xmat_test, radius)\n",
    "        X_combo = apply_noise(X_blur, std=noise_std, seed=combo_seed)\n",
    "        X_test_hog = extract_hog_features(X_combo)\n",
    "        y_pred = model_hog.predict(X_test_hog)\n",
    "        probs = model_hog.predict_proba(X_test_hog)\n",
    "        store_results(results, y_test_enc, y_pred, probs=probs, blur=radius, noise=noise_std, hog=True)\n",
    "\n",
    "# Show progress\n",
    "#df = pd.DataFrame(results)\n",
    "#print(\"\\n=== After Blur + Noise + HOG Augmentations ===\")\n",
    "#cols = [\n",
    "#    \"HOG\", \"PCA\", \"Blur\", \"Noise\",\n",
    "#    \"accuracy\", \"f1\", \"precision\", \"recall\",\n",
    "#    \"f1_class_0\", \"f1_class_1\", \"f1_class_2\", \"f1_class_3\",\n",
    "#    \"precision_class_0\", \"precision_class_1\", \"precision_class_2\", \"precision_class_3\",\n",
    "#    \"recall_class_0\", \"recall_class_1\", \"recall_class_2\", \"recall_class_3\",\n",
    "#    \"avg_conf\", \"avg_conf_class_0\", \"avg_conf_class_1\", \"avg_conf_class_2\", \"avg_conf_class_3\",\n",
    "#    \"count_pred_class_0\", \"count_pred_class_1\", \"count_pred_class_2\", \"count_pred_class_3\"\n",
    "#]\n",
    "\n",
    "#display(df[cols].sort_values(by=[\"PCA\", \"Blur\", \"Noise\"]).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d9db9-9bb8-4698-962a-3dd9f64d9e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"RF_results_100.csv\", index=False) # update to 100 here and above as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
